{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_Imdb_review.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOO+NxJFvo+9MCnYLbSQboZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shi-star/LSTM_IMDB_review/blob/main/LSTM_Imdb_review.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Yzyvuf-0Jyu"
      },
      "source": [
        "from keras.datasets import imdb #Dataset of imdb\n",
        "from keras.models import Sequential #Sequential the model \n",
        "from keras.layers import Dense  #Simple neural network layer\n",
        "from keras.layers import LSTM   #sequence processing\n",
        "from keras.layers.embeddings import Embedding  #to convert words into vectors fro processing\n",
        "from keras.preprocessing import  sequence # for zero padding of small sentence so that it will be able to create batches of sequence of sentences of different length\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-9sZiBa6OVU"
      },
      "source": [
        "**Imported the required libraries and packages.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuVUHF_M1z2c"
      },
      "source": [
        "top_words = 5000\n",
        "(X_train, y_train),  (X_test, y_test) = imdb.load_data(num_words=top_words)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfqknEM16T20"
      },
      "source": [
        "**We are taking only 5000 unique words from imdb dataset available in the keras.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atDlPB3Z4bTo",
        "outputId": "f1860f56-2a0c-4b45-90d1-2633d93656d5"
      },
      "source": [
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25000,)\n",
            "(25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxvWQY106ayY"
      },
      "source": [
        "**Let's check the shape of the X_train, X_test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1n8dKKw456E3",
        "outputId": "b8699c54-a8b6-404e-f4b9-d9a3cdf98b8f"
      },
      "source": [
        "print(X_train[0])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja5xMZbB6hKo"
      },
      "source": [
        "**Here is the first sentence of our train data, however the words in the sentence has been presented as the index number of them.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGrtTJF46CXm"
      },
      "source": [
        "unique_word_dictionary = imdb.get_word_index()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-8CmzDy99eh"
      },
      "source": [
        "**Got from imdb. to check the index number of each word.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulIpD6yi7jXM"
      },
      "source": [
        "reverse_index_word_dictionary = {}\n",
        "for i in unique_word_dictionary:\n",
        "  if not unique_word_dictionary[i] in reverse_index_word_dictionary:\n",
        "    reverse_index_word_dictionary[unique_word_dictionary[i]] = i"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPgxdnoQ-Nww"
      },
      "source": [
        "**Created the reverse dictionary so that we can check from index to words.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfUsa6Ci7-tb"
      },
      "source": [
        "reverse_index_word_dictionary\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9DFKILVSGky"
      },
      "source": [
        "We will see such type of dictionary, didn't show the output in notebook as it is such huge dictionary. And takes a lot of space.\n",
        "\n",
        "12998: 'aragorn',<br>\n",
        " 52272: 'hawker',<br>\n",
        " 52273: 'hawkes',<br>\n",
        " 3975: 'explosions',<br>\n",
        " 8059: 'loren',<br>\n",
        " 52274: \"pyle's\",<br>\n",
        " 52274: \"pyle's\",<br>\n",
        " 6704: 'shootout',<br>\n",
        " 18517: \"mike's\",<br>\n",
        " 52275: \"driscoll's\",<br>\n",
        " 52275: \"driscoll's\",<br>\n",
        " 40935: 'cogsworth',<br>\n",
        " 52276: \"britian's\",<br>\n",
        " 34744: 'childs',<br>\n",
        " 52277: \"portrait's\",<br>\n",
        " 3626: 'chain',<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g051kMtp-WUI"
      },
      "source": [
        "**We can see the first sentence.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDWIcTTy7yhV",
        "outputId": "41099aa1-38ed-4784-9f5f-039b4e0d758c"
      },
      "source": [
        "for i in X_train[0]:\n",
        "  print(reverse_index_word_dictionary[i])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the\n",
            "as\n",
            "you\n",
            "with\n",
            "out\n",
            "themselves\n",
            "powerful\n",
            "lets\n",
            "loves\n",
            "their\n",
            "becomes\n",
            "reaching\n",
            "had\n",
            "journalist\n",
            "of\n",
            "lot\n",
            "from\n",
            "anyone\n",
            "to\n",
            "have\n",
            "after\n",
            "out\n",
            "atmosphere\n",
            "never\n",
            "more\n",
            "room\n",
            "and\n",
            "it\n",
            "so\n",
            "heart\n",
            "shows\n",
            "to\n",
            "years\n",
            "of\n",
            "every\n",
            "never\n",
            "going\n",
            "and\n",
            "help\n",
            "moments\n",
            "or\n",
            "of\n",
            "every\n",
            "chest\n",
            "visual\n",
            "movie\n",
            "except\n",
            "her\n",
            "was\n",
            "several\n",
            "of\n",
            "enough\n",
            "more\n",
            "with\n",
            "is\n",
            "now\n",
            "current\n",
            "film\n",
            "as\n",
            "you\n",
            "of\n",
            "mine\n",
            "potentially\n",
            "unfortunately\n",
            "of\n",
            "you\n",
            "than\n",
            "him\n",
            "that\n",
            "with\n",
            "out\n",
            "themselves\n",
            "her\n",
            "get\n",
            "for\n",
            "was\n",
            "camp\n",
            "of\n",
            "you\n",
            "movie\n",
            "sometimes\n",
            "movie\n",
            "that\n",
            "with\n",
            "scary\n",
            "but\n",
            "and\n",
            "to\n",
            "story\n",
            "wonderful\n",
            "that\n",
            "in\n",
            "seeing\n",
            "in\n",
            "character\n",
            "to\n",
            "of\n",
            "70s\n",
            "and\n",
            "with\n",
            "heart\n",
            "had\n",
            "shadows\n",
            "they\n",
            "of\n",
            "here\n",
            "that\n",
            "with\n",
            "her\n",
            "serious\n",
            "to\n",
            "have\n",
            "does\n",
            "when\n",
            "from\n",
            "why\n",
            "what\n",
            "have\n",
            "critics\n",
            "they\n",
            "is\n",
            "you\n",
            "that\n",
            "isn't\n",
            "one\n",
            "will\n",
            "very\n",
            "to\n",
            "as\n",
            "itself\n",
            "with\n",
            "other\n",
            "and\n",
            "in\n",
            "of\n",
            "seen\n",
            "over\n",
            "and\n",
            "for\n",
            "anyone\n",
            "of\n",
            "and\n",
            "br\n",
            "show's\n",
            "to\n",
            "whether\n",
            "from\n",
            "than\n",
            "out\n",
            "themselves\n",
            "history\n",
            "he\n",
            "name\n",
            "half\n",
            "some\n",
            "br\n",
            "of\n",
            "and\n",
            "odd\n",
            "was\n",
            "two\n",
            "most\n",
            "of\n",
            "mean\n",
            "for\n",
            "1\n",
            "any\n",
            "an\n",
            "boat\n",
            "she\n",
            "he\n",
            "should\n",
            "is\n",
            "thought\n",
            "and\n",
            "but\n",
            "of\n",
            "script\n",
            "you\n",
            "not\n",
            "while\n",
            "history\n",
            "he\n",
            "heart\n",
            "to\n",
            "real\n",
            "at\n",
            "and\n",
            "but\n",
            "when\n",
            "from\n",
            "one\n",
            "bit\n",
            "then\n",
            "have\n",
            "two\n",
            "of\n",
            "script\n",
            "their\n",
            "with\n",
            "her\n",
            "nobody\n",
            "most\n",
            "that\n",
            "with\n",
            "wasn't\n",
            "to\n",
            "with\n",
            "armed\n",
            "acting\n",
            "watch\n",
            "an\n",
            "for\n",
            "with\n",
            "and\n",
            "film\n",
            "want\n",
            "an\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9YjlPr-8fm7",
        "outputId": "1abfcce5-45c9-444c-f49f-95dfc1875c2d"
      },
      "source": [
        "len(X_train[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "218"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jS2svkqV-bvp"
      },
      "source": [
        "**We can see the length of the first sentence is 218 words.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfeBbgLQ8qQS"
      },
      "source": [
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-6Mfx2v-gsS"
      },
      "source": [
        "**Here we have taken only the sentence having max words 500 and less than 500. so all the sentences which have more than 500 words in them, will be truncated and having less than 500 words, will get zero padded.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MK7UdKkJ-0Ht",
        "outputId": "34d053b8-a2a1-4d6f-cc63-b781d1e51c3d"
      },
      "source": [
        "print(X_train[0])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    1   14   22   16   43  530  973 1622 1385   65  458 4468\n",
            "   66 3941    4  173   36  256    5   25  100   43  838  112   50  670\n",
            "    2    9   35  480  284    5  150    4  172  112  167    2  336  385\n",
            "   39    4  172 4536 1111   17  546   38   13  447    4  192   50   16\n",
            "    6  147 2025   19   14   22    4 1920 4613  469    4   22   71   87\n",
            "   12   16   43  530   38   76   15   13 1247    4   22   17  515   17\n",
            "   12   16  626   18    2    5   62  386   12    8  316    8  106    5\n",
            "    4 2223    2   16  480   66 3785   33    4  130   12   16   38  619\n",
            "    5   25  124   51   36  135   48   25 1415   33    6   22   12  215\n",
            "   28   77   52    5   14  407   16   82    2    8    4  107  117    2\n",
            "   15  256    4    2    7 3766    5  723   36   71   43  530  476   26\n",
            "  400  317   46    7    4    2 1029   13  104   88    4  381   15  297\n",
            "   98   32 2071   56   26  141    6  194    2   18    4  226   22   21\n",
            "  134  476   26  480    5  144   30    2   18   51   36   28  224   92\n",
            "   25  104    4  226   65   16   38 1334   88   12   16  283    5   16\n",
            " 4472  113  103   32   15   16    2   19  178   32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaiWkj88_PRZ"
      },
      "source": [
        "**We can see the first sentence is of 218 length, so as it is less than 500 so zero padding has been applied here.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7BmBTbQ_EQt",
        "outputId": "ebfb42fd-60cf-4839-dc34-04b894594a3b"
      },
      "source": [
        "embedding_vector_length = 32 # Embedding vector is used by the embedding layer internally\n",
        "model = Sequential()\n",
        "model.add(Embedding(top_words, embedding_vector_length, input_length=500))\n",
        "model.add(LSTM(100))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 500, 32)           160000    \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UfL5EEhVNwtj"
      },
      "source": [
        "**we have built our model now.** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQU9ijDlNwqk"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4XV7saOAtRQ",
        "outputId": "f0222fd8-c947-4e14-e6b9-dbe6573667d9"
      },
      "source": [
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "391/391 [==============================] - 284s 680ms/step - loss: 0.5771 - accuracy: 0.6804 - val_loss: 0.3493 - val_accuracy: 0.8553\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 266s 681ms/step - loss: 0.3121 - accuracy: 0.8745 - val_loss: 0.3491 - val_accuracy: 0.8544\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 266s 682ms/step - loss: 0.2674 - accuracy: 0.8974 - val_loss: 0.3443 - val_accuracy: 0.8523\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 266s 679ms/step - loss: 0.2319 - accuracy: 0.9137 - val_loss: 0.3247 - val_accuracy: 0.8642\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 267s 684ms/step - loss: 0.2072 - accuracy: 0.9226 - val_loss: 0.3528 - val_accuracy: 0.8616\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f79d0551190>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KAWxDlxP01H"
      },
      "source": [
        "**We have fit the model.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLvMwXVTFWXK",
        "outputId": "039b0077-2b09-42f4-abc0-04c754787e1a"
      },
      "source": [
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 86.16%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLr7q9mvP32Z"
      },
      "source": [
        "**We can see the accuracy, We can try other things to increase the accuracy, like adding layers or increasing the neurons in the LSTM.** <br>\n",
        "However at this point it is just for learning purpose , I am okay with 86% accuracy:)"
      ]
    }
  ]
}